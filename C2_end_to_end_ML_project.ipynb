{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample End-to-End Machine Learning Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective : To predict the median housing price in any distict\n",
    "# Notes :: \n",
    "# 1) I only include essential codes that are necessary for the execution of the ML algorithmn\n",
    "# 2) Codes that are data explortary in nature are excluded and examined in another file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "housing = pd.read_csv(r\"C:\\Users\\tanzh\\Documents\\AI & Machine Learning\\Hands-On Machine Learning Codes\\Datasets\\housing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                41.0        880.0           129.0   \n",
       "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
       "2    -122.24     37.85                52.0       1467.0           190.0   \n",
       "3    -122.25     37.85                52.0       1274.0           235.0   \n",
       "4    -122.25     37.85                52.0       1627.0           280.0   \n",
       "5    -122.25     37.85                52.0        919.0           213.0   \n",
       "6    -122.25     37.84                52.0       2535.0           489.0   \n",
       "7    -122.25     37.84                52.0       3104.0           687.0   \n",
       "8    -122.26     37.84                42.0       2555.0           665.0   \n",
       "9    -122.25     37.84                52.0       3549.0           707.0   \n",
       "\n",
       "   population  households  median_income  median_house_value ocean_proximity  \n",
       "0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n",
       "1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  \n",
       "2       496.0       177.0         7.2574            352100.0        NEAR BAY  \n",
       "3       558.0       219.0         5.6431            341300.0        NEAR BAY  \n",
       "4       565.0       259.0         3.8462            342200.0        NEAR BAY  \n",
       "5       413.0       193.0         4.0368            269700.0        NEAR BAY  \n",
       "6      1094.0       514.0         3.6591            299200.0        NEAR BAY  \n",
       "7      1157.0       647.0         3.1200            241400.0        NEAR BAY  \n",
       "8      1206.0       595.0         2.0804            226700.0        NEAR BAY  \n",
       "9      1551.0       714.0         3.6912            261100.0        NEAR BAY  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>longitude</th>\n      <th>latitude</th>\n      <th>housing_median_age</th>\n      <th>total_rooms</th>\n      <th>total_bedrooms</th>\n      <th>population</th>\n      <th>households</th>\n      <th>median_income</th>\n      <th>median_house_value</th>\n      <th>ocean_proximity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>-122.23</td>\n      <td>37.88</td>\n      <td>41.0</td>\n      <td>880.0</td>\n      <td>129.0</td>\n      <td>322.0</td>\n      <td>126.0</td>\n      <td>8.3252</td>\n      <td>452600.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>-122.22</td>\n      <td>37.86</td>\n      <td>21.0</td>\n      <td>7099.0</td>\n      <td>1106.0</td>\n      <td>2401.0</td>\n      <td>1138.0</td>\n      <td>8.3014</td>\n      <td>358500.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>-122.24</td>\n      <td>37.85</td>\n      <td>52.0</td>\n      <td>1467.0</td>\n      <td>190.0</td>\n      <td>496.0</td>\n      <td>177.0</td>\n      <td>7.2574</td>\n      <td>352100.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>-122.25</td>\n      <td>37.85</td>\n      <td>52.0</td>\n      <td>1274.0</td>\n      <td>235.0</td>\n      <td>558.0</td>\n      <td>219.0</td>\n      <td>5.6431</td>\n      <td>341300.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>-122.25</td>\n      <td>37.85</td>\n      <td>52.0</td>\n      <td>1627.0</td>\n      <td>280.0</td>\n      <td>565.0</td>\n      <td>259.0</td>\n      <td>3.8462</td>\n      <td>342200.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>-122.25</td>\n      <td>37.85</td>\n      <td>52.0</td>\n      <td>919.0</td>\n      <td>213.0</td>\n      <td>413.0</td>\n      <td>193.0</td>\n      <td>4.0368</td>\n      <td>269700.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>-122.25</td>\n      <td>37.84</td>\n      <td>52.0</td>\n      <td>2535.0</td>\n      <td>489.0</td>\n      <td>1094.0</td>\n      <td>514.0</td>\n      <td>3.6591</td>\n      <td>299200.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>-122.25</td>\n      <td>37.84</td>\n      <td>52.0</td>\n      <td>3104.0</td>\n      <td>687.0</td>\n      <td>1157.0</td>\n      <td>647.0</td>\n      <td>3.1200</td>\n      <td>241400.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>-122.26</td>\n      <td>37.84</td>\n      <td>42.0</td>\n      <td>2555.0</td>\n      <td>665.0</td>\n      <td>1206.0</td>\n      <td>595.0</td>\n      <td>2.0804</td>\n      <td>226700.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>-122.25</td>\n      <td>37.84</td>\n      <td>52.0</td>\n      <td>3549.0</td>\n      <td>707.0</td>\n      <td>1551.0</td>\n      <td>714.0</td>\n      <td>3.6912</td>\n      <td>261100.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "housing.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Training and Test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will be using a technique called stratified sampling \n",
    "# to avoid sampling bias, the training and test data sets must be representative of the overall population\n",
    "# suppose MEDIAN INCOME is an important attribute that explains MEDIAN PRICES of houses\n",
    "# we then want to ensure that data composition of the training and test datasets, using MEDIAN INCOME as the criteria, is representative of the overall population "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we first create a calculated attribute called income cat\n",
    "\n",
    "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"], bins=[0.,1.5,3.0,4.5,6.,np.inf], labels= [1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   income_cat  median_income\n",
       "0           5         8.3252\n",
       "1           5         8.3014\n",
       "2           5         7.2574\n",
       "3           4         5.6431\n",
       "4           3         3.8462\n",
       "5           3         4.0368\n",
       "6           3         3.6591\n",
       "7           3         3.1200\n",
       "8           2         2.0804\n",
       "9           3         3.6912\n",
       "10          3         3.2031\n",
       "11          3         3.2705\n",
       "12          3         3.0750\n",
       "13          2         2.6736\n",
       "14          2         1.9167\n",
       "15          2         2.1250\n",
       "16          2         2.7750\n",
       "17          2         2.1202\n",
       "18          2         1.9911\n",
       "19          2         2.6033\n",
       "20          1         1.3578"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>income_cat</th>\n      <th>median_income</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>5</td>\n      <td>8.3252</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>5</td>\n      <td>8.3014</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>5</td>\n      <td>7.2574</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>4</td>\n      <td>5.6431</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>3</td>\n      <td>3.8462</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>3</td>\n      <td>4.0368</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>3</td>\n      <td>3.6591</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>3</td>\n      <td>3.1200</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>2</td>\n      <td>2.0804</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>3</td>\n      <td>3.6912</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>3</td>\n      <td>3.2031</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>3</td>\n      <td>3.2705</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>3</td>\n      <td>3.0750</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>2</td>\n      <td>2.6736</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>2</td>\n      <td>1.9167</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>2</td>\n      <td>2.1250</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>2</td>\n      <td>2.7750</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>2</td>\n      <td>2.1202</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>2</td>\n      <td>1.9911</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>2</td>\n      <td>2.6033</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1</td>\n      <td>1.3578</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "housing.loc[0:20,[\"income_cat\", \"median_income\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "# n_splits refer to the Number of re-shuffling & splitting iterations.\n",
    "\n",
    "for train_index, test_index in split.split(housing, housing[\"income_cat\"]) : \n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index]\n",
    "\n",
    "# both the training and test datasets has income category proportion almost identical to the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We then remove the income_cat column from the training and test datasets\n",
    "\n",
    "for set_ in (strat_train_set, strat_test_set) : \n",
    "    set_.drop(\"income_cat\", axis = 1, inplace = True)\n",
    "\n",
    "# axis = 1 refer to the column items, if equal to 0, it will refer to the row items\n",
    "# inplace = True means that the removal take place at the variable level\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing a clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to seperate the dependent variable and the independent variables\n",
    "\n",
    "housing = strat_train_set.drop(\"median_house_value\", axis = 1) # the drop method creates a copy of the data and does not impact strat_train_set\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy() # this only contain the dependent variable [median_house_value]\n",
    "\n",
    "# NOTE THAT HOUSING VARIABLE NOW ONLY CONTAIN THE TRAINING SET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Data Cleaning - Numercial Attributes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most ML cannot work well with missing attributes. we have the following options \n",
    "#1 get rid of the feature/column with missing entries \n",
    "#2 set the missing entries values to some values (0, median, mean etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we first create a copy of the data without categorical/text attributes as the technique that we will be using can only be applied on numerical attributes \n",
    "housing_num = housing.drop(\"ocean_proximity\", axis=1) # this creates a copy of the data without the categorical columns\n",
    "\n",
    "# we will be using a technique called SimpleImputer which allow us to fill in the missing entries with some strategy (mean/median/etc)\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy= \"median\") # define the strategy to a variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
       "              missing_values=nan, strategy='median', verbose=0)"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "imputer.fit(housing_num) # this will compute the median of each column attributes and store the results in its STATISTICS_instance variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([-118.51  ,   34.26  ,   29.    , 2119.5   ,  433.    , 1164.    ,\n",
       "        408.    ,    3.5409])"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nInt64Index: 16512 entries, 17606 to 15775\nData columns (total 8 columns):\nlongitude             16512 non-null float64\nlatitude              16512 non-null float64\nhousing_median_age    16512 non-null float64\ntotal_rooms           16512 non-null float64\ntotal_bedrooms        16354 non-null float64\npopulation            16512 non-null float64\nhouseholds            16512 non-null float64\nmedian_income         16512 non-null float64\ndtypes: float64(8)\nmemory usage: 1.1 MB\n"
     ]
    }
   ],
   "source": [
    "# Before we transform, glance at the number of missing entries in total_bedroom\n",
    "# there are 16,354 entries in the total_bedrooms column while there are 16,512 entires for the rest of the columns\n",
    "housing_num.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now want to use the trained imputer to transform the training set by replacing the missing vaues with the learned medians\n",
    "X = imputer.transform(housing_num) # this will return plain NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nInt64Index: 16512 entries, 17606 to 15775\nData columns (total 8 columns):\nlongitude             16512 non-null float64\nlatitude              16512 non-null float64\nhousing_median_age    16512 non-null float64\ntotal_rooms           16512 non-null float64\ntotal_bedrooms        16512 non-null float64\npopulation            16512 non-null float64\nhouseholds            16512 non-null float64\nmedian_income         16512 non-null float64\ndtypes: float64(8)\nmemory usage: 1.1 MB\n"
     ]
    }
   ],
   "source": [
    "# to put it back into a pandas dataframe\n",
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns, index=housing_num.index)\n",
    "housing_tr.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Data Cleaning - Text / Categorical Attrbutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we first create a column that contains only the attributes with categorical/text attributes\n",
    "housing_cat = housing[[\"ocean_proximity\"]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will be using a technique called one-hot encoding which create one binary attribute per category\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "cat_encoder = OneHotEncoder()\n",
    "\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat) # note that we are fitting the training data here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<16512x5 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 16512 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "housing_cat_1hot # the output is a SciPy sparse matrix where only the 1s are stored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with few exceptions, most ML algorithmns do not perform well when the input attributes have very different numercial scales\n",
    "# there are two common methods to scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Min-Max Scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also know as normalisation\n",
    "# values are shifted such that it will range-bound between 0 and 1 \n",
    "# this technique will be imapcted by extreme values on both ends\n",
    "# formula : [x(i) - min(x)] / [max(x) - min(x)]\n",
    "# the transformer in SL is known as MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Standardisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this technique does not range-bound the values which may pose a problem for some ML algorithmns\n",
    "# this technique is also less imapcted by extreme values on both ends\n",
    "# formula : [x(i) - mean(x)] / std(x)\n",
    "# the transformer in SL is known as StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation Pipeline for Numerical Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all transformation, only fit the scaler to the training data only\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "                        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "                        (\"std_scaler\", StandardScaler()),\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation Pipeline for Categorical Attributes & Full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "num_attribute = list(housing_num) # housing_num is the variable with the missing entries under the column < total_bedroooms > \n",
    "cat_attribute = [\"ocean_proximity\"]\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "                                  (\"num\", num_pipeline, num_attribute), # tjis will return a dense matrix\n",
    "                                  (\"cat\", OneHotEncoder(), cat_attribute), # this will return a sparse matric\n",
    "                                  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_prepared = full_pipeline.fit_transform(housing) #housing is the amended dataset that contain only the input features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluating on Training Dataset (Linear Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "# the first model we are testing on is the Linear Regression Model\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression() # we are assigning the linear regression function to a variable \n",
    "\n",
    "lin_reg.fit(housing_prepared, housing_labels) # the parameter is the [Dependent Variable, Independent Variable] # this is for training the model\n",
    "# this fit the training data and the labels using linear regression\n",
    "# after this step, we will have a working linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing on samples from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[211574.39523833 321345.10513719 210947.519838    61921.01197837\n 192362.32961119]\n"
     ]
    }
   ],
   "source": [
    "some_data = housing.iloc[:5] # to assign some data points from the training set\n",
    "some_labels = housing_labels[:5] # to assign some data points from the solutions, i.e. the dependent variable\n",
    "some_data_prepared = full_pipeline.transform(some_data)\n",
    "some_data_prediction = lin_reg.predict(some_data_prepared)\n",
    "\n",
    "print(some_data_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[286600.0, 340600.0, 196900.0, 46300.0, 254500.0]\n"
     ]
    }
   ],
   "source": [
    "print(list(some_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSE on the Linear Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "69050.98178244587\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "housing_prediction = lin_reg.predict(housing_prepared) # making prediction, the parameter is the training dataset\n",
    "lin_mse = mean_squared_error(housing_labels,housing_prediction) # the parameters are solutions and predictions\n",
    "lin_rmse = np.sqrt(lin_mse) # rmse is derived by taking the square root of mean square error (MSE)\n",
    "print(lin_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the prediction error is $69,050 \n",
    "# this is an example of a model underfitting the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluating on Training Dataset (Decision Tree Regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      presort=False, random_state=None, splitter='best')"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "tree_reg.fit(housing_prepared, housing_labels) # this line is for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "housing_prediction_tree = tree_reg.predict(housing_prepared) # making prediction\n",
    "tree_mse = mean_squared_error(housing_labels, housing_prediction_tree) # generating the mse using the dependent variable and the predictions\n",
    "tree_rmse = np.sqrt(tree_mse) \n",
    "print(tree_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have overfitted the model very badly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation on Decision Tree Regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one way to evaluate the Decision Tree Model will be to use the train_test_split() function to split the training dataset into two seperate datasets\n",
    "# the first set is a smaller dataset from the inital training dataset, and the second set is the validation set\n",
    "# we then train the model using the full training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an alternative method is technique called the k-fold cross-validation \n",
    "#1 the code split the training dataset into x distinct subsets (folds)\n",
    "#2 it train and evaluate the Decision Tree model x times\n",
    "#3 in each of the x iteration, it pick a different fold for evaluation while training on other 9 folds\n",
    "#4 the result is an array containing the 10 evaluation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will be using the k-fold cross validation technique below \n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(tree_reg, housing_prepared, housing_labels,scoring = \"neg_mean_squared_error\", cv = 10) # this generates 10 MSE scores\n",
    "# cv refer to the number of folds\n",
    "\n",
    "tree_rmse_scores = np.sqrt(-scores) # this generates 10 RMSE scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scores_stats(lst) : \n",
    "    print(\"min is \" + str(min(lst)))\n",
    "    print(\"max is \" + str(max(lst)))\n",
    "\n",
    "    print(\"mean is \" + str(np.mean(lst)))\n",
    "    print(\"std is \" +  str(np.std(lst)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[66960.64859997 66083.01931244 72665.52826216 69775.96933289\n 68721.7404781  76383.98569033 67568.71729512 68764.37811774\n 71566.8303965  69198.41115606]\n"
     ]
    }
   ],
   "source": [
    "print(tree_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it seems that the Decision Tree Model perform worse than the Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "min is 66083.01931244174\nmax is 76383.98569032672\nmean is 69768.92286413105\nstd is 2900.453046702138\n"
     ]
    }
   ],
   "source": [
    "display_scores_stats(tree_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation on Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "min is 65361.14176205049\nmax is 74639.88837894418\nmean is 69223.18594556303\nstd is 2657.268311277693\n"
     ]
    }
   ],
   "source": [
    "scores_lin = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring = \"neg_mean_squared_error\", cv = 10) \n",
    "scores_lin_rmse = np.sqrt(-scores_lin)\n",
    "\n",
    "display_scores_stats(scores_lin_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation on Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "                      max_features='auto', max_leaf_nodes=None,\n",
       "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                      min_samples_leaf=1, min_samples_split=2,\n",
       "                      min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                      n_jobs=None, oob_score=False, random_state=None,\n",
       "                      verbose=0, warm_start=False)"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "forest_reg = RandomForestRegressor()\n",
    "forest_reg.fit(housing_prepared, housing_labels) # this is to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "min is 48923.992301484315\nmax is 55515.12827198866\nmean is 52252.68040376816\nstd is 1801.5139882157175\n"
     ]
    }
   ],
   "source": [
    "forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels, scoring= \"neg_mean_squared_error\", cv = 10)\n",
    "forest_scores_rmse = np.sqrt(-forest_scores)\n",
    "\n",
    "display_scores_stats(forest_scores_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest work by training many Decision Tree models on random subsets of features and averaging out their predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([50841.80882396, 48923.99230148, 50874.63485223, 52531.57090896,\n",
       "       51960.62775258, 55515.12827199, 52723.49175231, 54075.72096227,\n",
       "       53758.11090236, 51321.71750952])"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "forest_scores_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning The Models - GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you define which parameters you want to experiment and the values to try out\n",
    "# it will then use cross-validation to evaluate all the possible combinations of hyperparameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [ {'n_estimators' : [3,10,30], 'max_features' : [2,4,6,8]}, \n",
    "               {'bootstrap' : [False], 'n_estimators' : [3,10], 'max_features': [2,3,4]},\n",
    "             ]\n",
    "\n",
    "# in the first dict, there are 12 combinations --> first group of specification\n",
    "# in the second dict, there are 6 combinations --> second group of specification\n",
    "\n",
    "# n_estimators and max_features are hyperparameters specified in the first dict\n",
    "# param_grid evaluate all 3 * 4 = 12 combinations of n_estimators and max_features hyperparameter values specified in the first dict\n",
    "# then, param_grid evaluate all 2 * 3 = 6 combinations of n_estimators and max_features hyperparameter values in the second dict, but with the bootstrap parameter set to False instead of True (which is the default value for this parameter)\n",
    "\n",
    "# In two iteration, there will be a total of 18 combination of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=RandomForestRegressor(bootstrap=True, criterion='mse',\n",
       "                                             max_depth=None,\n",
       "                                             max_features='auto',\n",
       "                                             max_leaf_nodes=None,\n",
       "                                             min_impurity_decrease=0.0,\n",
       "                                             min_impurity_split=None,\n",
       "                                             min_samples_leaf=1,\n",
       "                                             min_samples_split=2,\n",
       "                                             min_weight_fraction_leaf=0.0,\n",
       "                                             n_estimators='warn', n_jobs=None,\n",
       "                                             oob_score=False, random_state=None,\n",
       "                                             verbose=0, warm_start=False),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid=[{'max_features': [2, 4, 6, 8],\n",
       "                          'n_estimators': [3, 10, 30]},\n",
       "                         {'bootstrap': [False], 'max_features': [2, 3, 4],\n",
       "                          'n_estimators': [3, 10]}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "             scoring='neg_mean_squared_error', verbose=0)"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "forest_reg = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv = 5, scoring = \"neg_mean_squared_error\",return_train_score = True)\n",
    "# the grid search will explore the 18 combinations of RandomForestRegressor hyperparameter values and train each model 5 times \n",
    "# in total, there will be 90 rounds of trainings\n",
    "\n",
    "grid_search.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'max_features': 8, 'n_estimators': 30}"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "grid_search.best_params_ # this return the best combination of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "                      max_features=8, max_leaf_nodes=None,\n",
       "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                      min_samples_leaf=1, min_samples_split=2,\n",
       "                      min_weight_fraction_leaf=0.0, n_estimators=30,\n",
       "                      n_jobs=None, oob_score=False, random_state=None,\n",
       "                      verbose=0, warm_start=False)"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "grid_search.best_estimator_ # this return the best estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "63835.43014109018 {'max_features': 2, 'n_estimators': 3}\n55653.13133902398 {'max_features': 2, 'n_estimators': 10}\n52786.928567892784 {'max_features': 2, 'n_estimators': 30}\n60513.446655517495 {'max_features': 4, 'n_estimators': 3}\n53178.19451752226 {'max_features': 4, 'n_estimators': 10}\n50596.91200747191 {'max_features': 4, 'n_estimators': 30}\n59201.38025643767 {'max_features': 6, 'n_estimators': 3}\n52657.67378613478 {'max_features': 6, 'n_estimators': 10}\n50529.40189773309 {'max_features': 6, 'n_estimators': 30}\n58596.514006475496 {'max_features': 8, 'n_estimators': 3}\n52498.35753988611 {'max_features': 8, 'n_estimators': 10}\n50455.36662471176 {'max_features': 8, 'n_estimators': 30}\n62922.027106291716 {'bootstrap': False, 'max_features': 2, 'n_estimators': 3}\n54040.77958889584 {'bootstrap': False, 'max_features': 2, 'n_estimators': 10}\n60074.322098235956 {'bootstrap': False, 'max_features': 3, 'n_estimators': 3}\n52464.39397477669 {'bootstrap': False, 'max_features': 3, 'n_estimators': 10}\n58708.816971680346 {'bootstrap': False, 'max_features': 4, 'n_estimators': 3}\n52411.09205441438 {'bootstrap': False, 'max_features': 4, 'n_estimators': 10}\n"
     ]
    }
   ],
   "source": [
    "cvres = grid_search.cv_results_\n",
    "\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]) : \n",
    "    print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomized Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the grid search approach is fine when you are exploring relatively few combinations of hyperparameters\n",
    "# when the hyperparameters search space is large, it is better to use RandomizedSearchCV instead\n",
    "\n",
    "# Instead of trying out all the possible combination, RandomizedSearchCV evaluate a given number of random combination by selecting a random value for each hyperparameter at each iteration \n",
    "# if you let the RandomizedSearchCV run for 1000 iterations, this approach will explore 1000 different values for each hyperparameters instead of just a few values for each hyperparameters with the grid search method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing the best models and their errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1.14964944e-01, 1.02753179e-01, 5.10712766e-02, 2.88390604e-02,\n",
       "       2.73757360e-02, 3.59971155e-02, 2.43326732e-02, 4.45753633e-01,\n",
       "       1.10645301e-02, 1.48474815e-01, 5.51048534e-05, 1.98750350e-03,\n",
       "       7.33042970e-03])"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "# the RandomForestRegressor can indicate the relative importance of each attribute for making accurate predictions\n",
    "\n",
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(0.44575363301049215, 'median_income'),\n",
       " (0.14847481473535173, 'INLAND'),\n",
       " (0.11496494350292082, 'longitude'),\n",
       " (0.10275317883521921, 'latitude'),\n",
       " (0.0510712766306358, 'housing_median_age'),\n",
       " (0.03599711550984855, 'population'),\n",
       " (0.028839060438268605, 'total_rooms'),\n",
       " (0.02737573595727476, 'total_bedrooms'),\n",
       " (0.024332673198226448, 'households'),\n",
       " (0.011064530130259124, '<1H OCEAN'),\n",
       " (0.007330429696632295, 'NEAR OCEAN'),\n",
       " (0.001987503501489285, 'NEAR BAY'),\n",
       " (5.510485338123466e-05, 'ISLAND')]"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "cat_encoder = full_pipeline.named_transformers_[\"cat\"]\n",
    "cat_one_hot_attribs = list(cat_encoder.categories_[0])\n",
    "attributes = num_attribute + cat_one_hot_attribs\n",
    "\n",
    "sorted(zip(feature_importances, attributes), reverse = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating on Test System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at this point, run the full_pipeline to transform the data -- call the transform() function, NOT the fit_transform -- YOU DO NOT WANT TO FIT THE TEST DATA\n",
    "# we are noe evaluating the final model on the test set\n",
    "\n",
    "final_model = grid_search.best_estimator_ \n",
    "\n",
    "X_test = strat_test_set.drop(\"median_house_value\", axis = 1) # this contain the predictors / independent variables in the test dataset\n",
    "y_test = strat_test_set[\"median_house_value\"].copy()         # this contain the dependent variables in the test dataset\n",
    "\n",
    "X_test_prepared = full_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "47569.129976311844\n"
     ]
    }
   ],
   "source": [
    "final_predictions = final_model.predict(X_test_prepared)\n",
    "final_mse = mean_squared_error(y_test, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse)\n",
    "print(final_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Confidence Interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([45578.87786644, 49479.39112239])"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "# in some cases, a point estimate of the generalization error will not be quite enough \n",
    "# computing a 95% CI for the generalization error can help you to have an idea how precise this estimate is\n",
    "\n",
    "from scipy import stats\n",
    "confidence = 0.95\n",
    "\n",
    "squared_errors = (final_predictions - y_test) ** 2\n",
    "np.sqrt(stats.t.interval(confidence, len(squared_errors)-1, loc=squared_errors.mean(), scale = stats.sem(squared_errors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464bitbaseconda6db2893f0e1b4b6ea66d23044bec9473",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}