{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37464bitbaseconda6db2893f0e1b4b6ea66d23044bec9473",
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Ensemble Learning and Methods\n",
    "\n",
    "* Ensemble method is where we train multiple models\n",
    "* Predictions are made by aggregating the predictions from each model in the ensemble\n",
    "    *In a classification problem, the class with the most vote wins\n",
    "    * A majority-vote classifier is also called a hard-voting classifier \n",
    "* Even if each model is a weak learner, the ensemble can still be a strong learner provided that there are sufficient number of weak learners and they are sufficiently diverse\n",
    "* The above is true if:\n",
    "    * all the models are perfectly independent, making uncorrelated errors which may not be the case if they are trained on the same data\n",
    "\t* If they are trained on the same data, they will most likely make the same type of errors hence there will be many majority votes for the wrong class, reducing the ensemble's accuracy\n",
    "Ensemble methods work best when the models are as independent from one another as possible"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets as datasets\n",
    "import sklearn.ensemble as ensemble\n",
    "import sklearn.tree as tree\n",
    "import sklearn.linear_model as lm\n",
    "import sklearn.svm as svm\n",
    "import sklearn.metrics as metrics\n",
    "import sklearn.model_selection as ms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_classification(n_samples=15000, n_features=100, n_informative=25, n_redundant=10, n_classes=2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = ms.train_test_split(X, y, train_size= 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(15000, 100)"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lc', LogisticRegression(random_state=42)),\n",
       "                             ('rf', RandomForestClassifier(random_state=42)),\n",
       "                             ('svc', SVC(random_state=42))])"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "log_clf = lm.LogisticRegression(random_state=42)\n",
    "rf_clf = ensemble.RandomForestClassifier(random_state=42)\n",
    "svm_clf = svm.SVC(random_state=42) # currently do not have the predict_proba method and hence cannot output class probabilities\n",
    "\n",
    "voting_classifier = ensemble.VotingClassifier(estimators=[('lc', log_clf),\n",
    "                                ('rf', rf_clf),\n",
    "                                ('svc', svm_clf)], voting='hard')\n",
    "\n",
    "voting_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LogisticRegression : 78.86666666666666\n",
      "RandomForestClassifier : 92.60000000000001\n",
      "SVC : 96.8\n",
      "VotingClassifier : 93.76666666666667\n"
     ]
    }
   ],
   "source": [
    "classifer_list = (log_clf, rf_clf, svm_clf, voting_classifier)\n",
    "\n",
    "for classifier in classifer_list:\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_prediction_test = classifier.predict(X_test)\n",
    "    y_prediction_accuracy = metrics.accuracy_score(y_test, y_prediction_test)\n",
    "    print(f'{classifier.__class__.__name__} : {y_prediction_accuracy * 100}')"
   ]
  },
  {
   "source": [
    "* Hard-voting classifier (majority-vote classifier) is where we aggregate the predictions of each classifier and predict the class that gets the most vote\n",
    "* Soft-voting classifier is where we predict the class with the highest class probability, averaged over all the individual classifiers\n",
    "    * This often achieve higher performance than hard-voting becasue it give more weight to highly confident vote\n",
    "    * Soft-voting is only possible if all the classifiers are able to estimate class probabilities, i.e. they all have a predict_proba() method"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LogisticRegression(random_state=42) : 78.86666666666666\n",
      "RandomForestClassifier(random_state=42) : 92.60000000000001\n",
      "SVC(probability=True, random_state=42) : 96.8\n",
      "VotingClassifier(estimators=[('lc', LogisticRegression(random_state=42)),\n",
      "                             ('rf', RandomForestClassifier(random_state=42)),\n",
      "                             ('svc', SVC(probability=True, random_state=42))],\n",
      "                 voting='soft') : 95.36666666666666\n"
     ]
    }
   ],
   "source": [
    "log_clf = lm.LogisticRegression(random_state=42)\n",
    "rf_clf = ensemble.RandomForestClassifier(random_state=42)\n",
    "svm_clf = svm.SVC(random_state=42, probability=True) # this will create a predict_proba and can output class probabilities\n",
    "\n",
    "voting_classifier = ensemble.VotingClassifier(estimators=[('lc', log_clf),\n",
    "                                ('rf', rf_clf),\n",
    "                                ('svc', svm_clf)], voting='soft') # change to soft-voting method \n",
    "\n",
    "voting_classifier.fit(X_train, y_train)\n",
    "\n",
    "classifer_list = (log_clf, rf_clf, svm_clf, voting_classifier)\n",
    "for classifier in classifer_list:\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_prediction_test = classifier.predict(X_test)\n",
    "    y_prediction_accuracy = metrics.accuracy_score(y_test, y_prediction_test)\n",
    "    print(f'{classifier} : {y_prediction_accuracy * 100}')"
   ]
  },
  {
   "source": [
    "# Bagging and Pasting\n",
    "\n",
    "* One method to get a diverse set of models is to use the same set of training algorithms and train them on differnt random subsets of the training data\n",
    "* The random subsets of data is created via random sampling of the original dataset\n",
    "* If the random sampling is done with replacement (i.e. the same instance can be picked more than once), it is called Bagging\n",
    "* If the random sampling is done without replacement, it is called Pasting\n",
    "* Bootstrapping introduces a bit more diversity into the subsets where each model is trained on hence bagging will end up with a model with slightly higher bias than Pasting\n",
    "* However, the diversity means that the models in the ensemble will end up being less correlated with each other hence the variance is reduced"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.78583333 0.78333333 0.77041667 0.78541667 0.79291667 0.78458333\n 0.77083333 0.77916667 0.79666667 0.78166667 0.80041667 0.78875\n 0.785      0.77791667 0.78166667]\n78.43055555555554\n0.8028690836195516\n\nTesting Prediction Accuracy\n79.80000000000001\n"
     ]
    }
   ],
   "source": [
    "tree_clf = tree.DecisionTreeClassifier(random_state=42)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "\n",
    "cv = ms.RepeatedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "accuracy_score_cv = ms.cross_val_score(tree_clf, X_train, y_train, scoring='accuracy', cv=cv)\n",
    "print(accuracy_score_cv)\n",
    "print(np.mean(accuracy_score_cv) * 100)\n",
    "print(np.std(accuracy_score_cv) * 100)\n",
    "\n",
    "#testing prediction\n",
    "tree_clf_test_prediction = tree_clf.predict(X_test)\n",
    "tree_clf_test_accuracy = metrics.accuracy_score(y_test, tree_clf_test_prediction)\n",
    "print()\n",
    "print('Testing Prediction Accuracy')\n",
    "print(tree_clf_test_accuracy * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.79458333 0.785      0.78166667 0.77458333 0.77958333 0.78708333\n",
      " 0.78458333 0.78875    0.7825     0.77791667 0.78666667 0.7875\n",
      " 0.78875    0.78166667 0.7825    ]\n",
      "78.42222222222223\n",
      "0.4810931482404946\n",
      "\n",
      "oob score : 0.7835\n",
      "\n",
      "Testing Prediction Accuracy\n",
      "78.76666666666667\n"
     ]
    }
   ],
   "source": [
    "bag_clf = ensemble.BaggingClassifier(\n",
    "                            tree.DecisionTreeClassifier(random_state=42), \n",
    "                            n_estimators=500, # number of trees/models/predictors in the ensemble\n",
    "                            max_samples=100, # number of samples to be used in the bagging process to train each tree in the ensemble\n",
    "                            bootstrap=True,\n",
    "                            n_jobs=-1,\n",
    "                            random_state=42, \n",
    "                            oob_score= True)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "\n",
    "# the bagging classifier class will automatically use soft-voting method if the underlying estimator can estimate class probabilities\n",
    "\n",
    "cv = ms.RepeatedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "accuracy_score_cv = ms.cross_val_score(bag_clf, X_train, y_train, scoring='accuracy', cv=cv)\n",
    "print(accuracy_score_cv)\n",
    "print(np.mean(accuracy_score_cv) * 100)\n",
    "print(np.std(accuracy_score_cv) * 100)\n",
    "\n",
    "print()\n",
    "print(f'oob score : {bag_clf.oob_score_}')\n",
    "\n",
    "bag_clf_test_prediction = bag_clf.predict(X_test)\n",
    "bag_clf_test_accuracy = metrics.accuracy_score(y_test, bag_clf_test_prediction)\n",
    "print()\n",
    "print('Testing Prediction Accuracy')\n",
    "print(bag_clf_test_accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.47368421, 0.52631579],\n",
       "       [0.34747475, 0.65252525],\n",
       "       [0.38709677, 0.61290323],\n",
       "       ...,\n",
       "       [0.26060606, 0.73939394],\n",
       "       [0.72635815, 0.27364185],\n",
       "       [0.57228916, 0.42771084]])"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "bag_clf.oob_decision_function_"
   ]
  },
  {
   "source": [
    "* With bagging, some instances will not get picked by any of the models\n",
    "* The instances that are not picked are known as out-of-bag instances \n",
    "* If 60% of the training instances are sampled on average for each model, this means that 40% of the instances are the OOB instances\n",
    "* Note that they are not the same 40% across all the models in the ensemble\n",
    "* Since the OOB is not used during the training process, it can used to evaluate the model similar to the testing dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Random Patches and Random Subspace\n",
    "\n",
    "* Instead of sampling the instances, we can sample the input features as well\n",
    "* Random Subspace is a training method where we keep all the training instances and perform sampling ONLY on the input features\n",
    "* Random Patches is a training methiod where we perform random sampling on BOTH the training instances and the input features \n",
    "* Once again, random subspace is another technique for us to introduce more diversity into the model to reduce variance at the expense of potentially slightly higher bias \n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.80791667 0.79875    0.79125    0.78916667 0.79541667 0.80041667\n 0.78875    0.79791667 0.79375    0.79833333 0.79583333 0.80375\n 0.79791667 0.79375    0.7925    ]\n79.63611111111109\n0.5063181062250862\n\noob score : 0.7979166666666667\n\nTesting Prediction Accuracy\n79.4\n"
     ]
    }
   ],
   "source": [
    "new_rf_clf = ensemble.RandomForestClassifier(\n",
    "                                            n_estimators=500, \n",
    "                                            bootstrap=True, # random sampling of instances\n",
    "                                            max_samples=100, \n",
    "                                            max_features=25, # random sampling of input features\n",
    "                                            oob_score=True,\n",
    "                                            random_state=42, \n",
    "                                            n_jobs=-1\n",
    "                                            )\n",
    "new_rf_clf.fit(X_train, y_train)\n",
    "\n",
    "cv = ms.RepeatedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "accuracy_score_cv = ms.cross_val_score(new_rf_clf, X_train, y_train, scoring='accuracy', cv=cv)\n",
    "print(accuracy_score_cv)\n",
    "print(np.mean(accuracy_score_cv) * 100)\n",
    "print(np.std(accuracy_score_cv) * 100)\n",
    "\n",
    "print()\n",
    "print(f'oob score : {new_rf_clf.oob_score_}')\n",
    "\n",
    "new_bag_clf_test_prediction = new_rf_clf.predict(X_test)\n",
    "new_bag_clf_test_accuracy = metrics.accuracy_score(y_test, new_bag_clf_test_prediction)\n",
    "print()\n",
    "print('Testing Prediction Accuracy')\n",
    "print(new_bag_clf_test_accuracy*100)"
   ]
  },
  {
   "source": [
    "# Extra_Tree\n",
    "\n",
    "* The extremely randomized trees model is another model aims to reduce variance at the expense of higher bias"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.81833333 0.81208333 0.81041667 0.79208333 0.80125    0.81083333\n 0.80625    0.81       0.80291667 0.80291667 0.81041667 0.80375\n 0.79458333 0.81041667 0.79291667]\n80.52777777777777\n0.7398490002763247\n\noob score : 0.8014166666666667\n\nTesting Prediction Accuracy\n79.76666666666667\n"
     ]
    }
   ],
   "source": [
    "e_trees = ensemble.ExtraTreesClassifier(\n",
    "                                        n_estimators=500, \n",
    "                                        bootstrap=True, # random sampling of instances\n",
    "                                        max_samples=100, \n",
    "                                        max_features=25, # random sampling of input features\n",
    "                                        oob_score=True,\n",
    "                                        random_state=42, \n",
    "                                        n_jobs=-1\n",
    "                                        )\n",
    "e_trees.fit(X_train, y_train)\n",
    "\n",
    "cv = ms.RepeatedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "accuracy_score_cv = ms.cross_val_score(e_trees, X_train, y_train, scoring='accuracy', cv=cv)\n",
    "print(accuracy_score_cv)\n",
    "print(np.mean(accuracy_score_cv) * 100)\n",
    "print(np.std(accuracy_score_cv) * 100)\n",
    "\n",
    "print()\n",
    "print(f'oob score : {e_trees.oob_score_}')\n",
    "\n",
    "e_tree_clf_test_prediction = e_trees.predict(X_test)\n",
    "e_tree_clf_test_accuracy = metrics.accuracy_score(y_test, e_tree_clf_test_prediction)\n",
    "print()\n",
    "print('Testing Prediction Accuracy')\n",
    "print(e_tree_clf_test_accuracy*100)"
   ]
  },
  {
   "source": [
    "# AdaBoost Classifier\n",
    "\n",
    "* A base classifier model is trained and used to make predictions on the training dataset\n",
    "* From the predictions, INCREASE the weight of incorrectly classified instances\n",
    "* Trained a second classifier model with the updated weights and use it to make predictions on the training dataset \n",
    "etc. and etc..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.82541667 0.8325     0.8225     0.82666667 0.82083333 0.82791667\n",
      " 0.81041667 0.82625    0.82541667 0.82916667 0.82958333 0.82708333\n",
      " 0.8175     0.82458333 0.82375   ]\n",
      "82.46388888888887\n",
      "0.5192735904399485\n",
      "\n",
      "Testing Prediction Accuracy\n",
      "82.33333333333334\n"
     ]
    }
   ],
   "source": [
    "adaboo_clf = ensemble.AdaBoostClassifier(\n",
    "                                    base_estimator=tree.DecisionTreeClassifier(max_depth=1),\n",
    "                                    n_estimators=200, \n",
    "                                    algorithm='SAMME.R',\n",
    "                                    learning_rate=0.5, \n",
    "                                    random_state=42,        \n",
    "                                        )\n",
    "adaboo_clf.fit(X_train, y_train)\n",
    "\n",
    "cv = ms.RepeatedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "accuracy_score_cv = ms.cross_val_score(adaboo_clf, X_train, y_train, scoring='accuracy', cv=cv)\n",
    "print(accuracy_score_cv)\n",
    "print(np.mean(accuracy_score_cv) * 100)\n",
    "print(np.std(accuracy_score_cv) * 100)\n",
    "\n",
    "adaboo_clf_test_prediction = adaboo_clf.predict(X_test)\n",
    "adaboo_clf_test_accuracy = metrics.accuracy_score(y_test, adaboo_clf_test_prediction)\n",
    "print()\n",
    "print('Testing Prediction Accuracy')\n",
    "print(adaboo_clf_test_accuracy*100)"
   ]
  },
  {
   "source": [
    "# Gradient Boosting Classifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.8925     0.90166667 0.89333333 0.88666667 0.8925     0.89583333\n 0.89041667 0.88916667 0.88916667 0.89666667 0.89041667 0.90083333\n 0.89583333 0.89083333 0.89333333]\n89.32777777777777\n0.41075434970608515\n\nTesting Prediction Accuracy\n89.26666666666667\n"
     ]
    }
   ],
   "source": [
    "gb_clf = ensemble.GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "gb_clf.fit(X_train, y_train)\n",
    "\n",
    "cv = ms.RepeatedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "accuracy_score_cv = ms.cross_val_score(gb_clf, X_train, y_train, scoring='accuracy', cv=cv)\n",
    "print(accuracy_score_cv)\n",
    "print(np.mean(accuracy_score_cv) * 100)\n",
    "print(np.std(accuracy_score_cv) * 100)\n",
    "\n",
    "gb_clf_test_prediction = gb_clf.predict(X_test)\n",
    "gb_clf_test_accuracy = metrics.accuracy_score(y_test, gb_clf_test_prediction)\n",
    "print()\n",
    "print('Testing Prediction Accuracy')\n",
    "print(gb_clf_test_accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}